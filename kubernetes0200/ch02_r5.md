## Problem

You want to create a Kubernetes master node using a few Docker containers. Specifically, you want to run the API server, scheduler, controller, and `etcd` key/value store within containers.

## Solution

You can use the `hyperkube` binary plus an `etcd` container. `hyperkube` is an all-in-one binary available as a Docker image. You can use it to start all the Kubernetes processes.

To create a Kubernetes cluster, you need a storage solution to keep the cluster state. In Kubernetes, this solution is a distributed key/value store called `etcd`; therefore, first you need to start an etcd instance. You can run it like this:

$ **docker run -d \\
         --name=k8s \\
         -p 8080:8080 \\
         gcr.io/google\_containers/etcd:3.1.10 \\
         etcd --data-dir /var/lib/data**

Then you will start the API server using a so-called `hyperkube` _image_, which contains the API server binary. This image is available from the Google Container Registry (GCR) at `gcr.io/google_containers/hyperkube:v1.7.11`. We use a few settings to serve the API insecurely on a local port. Replace `v1.7.11` with the latest version or the one you want to run:

$ **docker run -d \\
         --net=container:k8s \\
         gcr.io/google\_containers/hyperkube:v1.7.11/ \\
         apiserver --etcd-servers=http://127.0.0.1:2379 \\
         --service-cluster-ip-range=10.0.0.1/24 \\
         --insecure-bind-address=0.0.0.0 \\
         --insecure-port=8080 \\
         --admission-control=AlwaysAdmit**

Finally, you can start the admission controller, which points to the API server:

$ **docker run -d \\
         --net=container:k8s \\
         gcr.io/google\_containers/hyperkube:v1.7.11/ \\
         controller-manager --master=127.0.0.1:8080**

Notice that since `etcd`, the API server, and the controller-manager share the same network namespace, they can reach each other on 127.0.0.1 even though they are running in different containers.

To test that you have a working setup, use `etcdctl` in the `etcd` container and list what is in the _/registry_ directory:

$ **docker exec -ti k8s /bin/sh**
# **export ETCDCTL\_API=3**
# **etcdctl get "/registry/api" --prefix=true**

You can also reach your Kubernetes API server and start exploring the API:

$ **curl -s curl http://127.0.0.1:8080/api/v1 | more**
{
  "kind": "APIResourceList",
  "groupVersion": "v1",
  "resources": \[
    {
      "name": "bindings",
      "singularName": "",
      "namespaced": true,
      "kind": "Binding",
      "verbs": \[
        "create"
      \]
    },
...

So far, you have not started the scheduler, nor have you set up nodes with the `kubelet` and the `kube-proxy`. This just shows you how you can run the Kubernetes API server by starting three local containers.

Tip

It is sometimes helpful to use the `hyperkube` Docker image to verify some of the configuration options of one of the Kubernetes binaries. For example, to check the help for the main `/apiserver` command, try:

$ **docker run --rm -ti \\
         gcr.io/google\_containers/hyperkube:v1.7.11 \\
         /apiserver --help**

## Discussion

Though this is a very useful way to start exploring the various Kubernetes components locally, it is not recommended for a production setup.

## See Also

*   [`hyperkube` Docker images](https://github.com/kubernetes/kubernetes/tree/master/cluster/images/hyperkube)