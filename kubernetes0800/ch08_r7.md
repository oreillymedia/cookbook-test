## Problem

You don’t want to lose data on a disk your container uses—​that is, you want to make sure it survives a restart of the hosting pod.

## Solution

Use a persistent volume (PV). In the case of Minikube, you can create a PV of type `hostPath` and mount it just like a normal volume into the container’s filesystem.

First, define the PV `hostpathpv` in a manifest called _hostpath-pv.yaml_:

```
kind:               PersistentVolume
apiVersion:         v1
metadata:
  name:             hostpathpv
  labels:
    type:           local
spec:
  storageClassName: manual
  capacity:
    storage:        1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path:           "/tmp/pvdata"
```{{execute}}

Before you can create the PV, however, you need to prepare the directory _/tmp/pvdata_ on the node—​that is, the Minikube instance itself. You can get into the node where the Kubernetes cluster is running using `minikube ssh`:

$ **minikube ssh**

$ **mkdir /tmp/pvdata && \\
  echo 'I am content served from a delicious persistent volume' > / \\
  tmp/pvdata/index.html**

$ **cat /tmp/pvdata/index.html**
I am content served from a delicious persistent volume

$ **exit**

Now that you’ve prepared the directory on the node, you can create the PV from the manifest file _hostpath-pv.yaml_:

$ **kubectl create -f hostpath-pv.yaml**
persistentvolume "hostpathpv" created

$ **kubectl get pv**
NAME        CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      ...   ...   ...
hostpathpv  1Gi        RWO           Retain          Available   ...   ...   ...

$ **kubectl describe pv/hostpathpv**
Name:           hostpathpv
Labels:         type=local
Annotations:    <none>
StorageClass:   manual
Status:         Available
Claim:
Reclaim Policy: Retain
Access Modes:   RWO
Capacity:       1Gi
Message:
Source:
    Type:       HostPath (bare host directory volume)
    Path:       /tmp/pvdata
Events:         <none>

Up to this point, you would carry out these steps in an admin role. You would define PVs and make them available to developers on the Kubernetes cluster.

Now you’re in a position to use the PV in a pod, from a developer’s perspective. This is done via a _persistent volume claim_ (PVC), so called because, well, you literally claim a PV that fulfills certain characteristics, such as size or storage class.

Create a manifest file called _pvc.yaml_ that defines a PVC, asking for 200 MB of space:

```
kind:               PersistentVolumeClaim
apiVersion:         v1
metadata:
  name:             mypvc
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage:      200Mi
```{{execute}}

Next, launch the PVC and verify its state:

$ **kubectl create -f pvc.yaml**
persistentvolumeclaim "mypvc" created

$ **kubectl get pv**
NAME        CAPACITY  ACCESSMODES  ...  STATUS  CLAIM          STORAGECLASS
hostpathpv  1Gi       RWO          ...  Bound   default/mypvc  manual

Note that the status of the PV `hostpathpv` has changed from `Available` to `Bound`.

Finally, it’s time to consume the data from the PV in a container, this time via a deployment that mounts it in the filesystem. So, create a file called _nginx-using-pv.yaml_ with the following content:

```
kind:                          Deployment
apiVersion:                    extensions/v1beta1
metadata:
  name:                        nginx-with-pv
spec:
  replicas:                    1
  template:
    metadata:
      labels:
        app:                   nginx
    spec:
      containers:
      - name:                  webserver
        image:                 nginx
        ports:
        - containerPort:       80
        volumeMounts:
        - mountPath:           "/usr/share/nginx/html"
          name:                webservercontent
      volumes:
      - name:                  webservercontent
        persistentVolumeClaim:
          claimName:           mypvc
```{{execute}}

And launch the deployment, like so:

$ **kubectl create -f nginx-using-pv.yaml**
deployment "nginx-with-pv" created

$ **kubectl get pvc**
NAME   STATUS  VOLUME      CAPACITY  ACCESSMODES  STORAGECLASS  AGE
mypvc  Bound   hostpathpv  1Gi       RWO          manual        12m

As you can see, the PV is in use via the PVC you created earlier.

To verify that the data actually has arrived, you could now create a service (see [\[simple\_service\]](#simple_service)) along with an `ingress` object (see [\[ingress\]](#ingress)) and then access it like so:

$ **curl -k -s https://192.168.99.100/web**
I am content served from a delicious persistent volume

Well done! You’ve (as an admin) provisioned a persistent volume and (as a developer) claimed it via a persistent volume claim, and used it from a deployment in a pod by mounting it into the container filesystem.

## Discussion

In the Solution, we used a persistent volume of type `hostPath`. In a production setting, you would not want to use this but rather ask your cluster administrator nicely to provision a networked volume backed by NFS or an Amazon Elastic Block Store (EBS) volume to make sure your data sticks around and survives single-node failures.

Note

Remember that PVs are cluster-wide resources; that is, they are not namespaced. However, PVCs are namespaced. You can claim PVs from specific namespaces using namespaced PVCs.

## See Also

*   Kubernetes [Persistent Volumes documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
    
*   [Configure a Pod to Use a PersistentVolume for Storage](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/)