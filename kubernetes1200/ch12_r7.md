## Problem

You need to add a worker node to your Kubernetes cluster.

## Solution

Provision a new machine in whatever way your environment requires (for example, in a bare-metal environment you might need to physically install a new server in a rack, in a public cloud setting you need to create a new VM, etc.), and then install the three components that make up a Kubernetes worker node:

`kubelet`

This is the node manager and supervisor for all pods, no matter if they’re controlled by the API server or running locally, such as static pods. Note that the `kubelet` is the final arbiter of what pods can or cannot run on a given node, and takes care of:

*   Reporting node and pod statuses to the API server.
    
*   Periodically executing liveness probes.
    
*   Mounting the pod volumes and downloading secrets.
    
*   Controlling the container runtime (see the following).
    

`Container runtime`

This is responsible for downloading container images and running the containers. Initially, this was hardwired to the Docker engine, but nowadays it is a pluggable system based on the [Container Runtime Interface (CRI)](https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md), so you can, for example, use [CRI-O](http://cri-o.io/) rather than Docker.

`kube-proxy`

This process dynamically configures iptables rules on the node to enable the Kubernetes service abstraction (redirecting the VIP to the endpoints, one or more pods representing the service).

The actual installation of the components depends heavily on your environment and the installation method used (cloud, `kubeadm`, etc.). For a list of available options, see the [kubelet reference](https://kubernetes.io/docs/admin/kubelet/) and [kube-proxy reference](https://kubernetes.io/docs/admin/kube-proxy/).

## Discussion

Worker nodes, unlike other Kubernetes resources such as a deployments or services, are not directly created by the Kubernetes control plane but only managed by it. That means when Kubernetes creates a node, it actually only creates an object that _represents_ the worker node. It validates the node by health checks based on the node’s `metadata.name` field, and if the node is valid—​that is, all necessary components are running—​it is considered part of the cluster; otherwise, it will be ignored for any cluster activity until it becomes valid.

## See Also

*   ["The Kubernetes Node"](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node) in the Kubernetes Architecture design document
    
*   [Master-Node communication](https://kubernetes.io/docs/concepts/architecture/master-node-communication/)
    
*   [Static Pods](https://kubernetes.io/docs/tasks/administer-cluster/static-pod/)