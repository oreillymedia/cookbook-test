## Problem

You want to be able to automatically check if your app is healthy and let Kubernetes take action if this is not the case.

## Solution

To signal to Kubernetes how your app is doing, add liveness and readiness probes as described here.

The starting point is a deployment manifest, _webserver.yaml_:

```
apiVersion:              extensions/v1beta1
kind:                    Deployment
metadata:
  name:                  webserver
spec:
  replicas:              1
  template:
    metadata:
      labels:
        app:             nginx
    spec:
      containers:
      - name:            nginx
        image:           nginx:stable
        ports:
        - containerPort: 80
```{{execute}}

Liveness and readiness probes are defined in the containers section of the pod specification. See the introductory examples ([Recover from a Broken State with a Liveness Probe](#liveness) and [Controlling Traffic Flow to a Pod Using a Readiness Probe](#readiness)) and add the following to your container spec in your deployment’s pod template:

```
...
     livenessProbe:
       initialDelaySecond: 2
       periodSeconds: 10
       httpGet:
         path: /
         port: 80
     readinessProbe:
       initialDelaySecond: 2
       periodSeconds: 10
       httpGet:
         path: /
         port: 80
...
```{{execute}}

And now you can launch it and check the probes:

$ **kubectl create -f webserver.yaml**

$ **kubectl get pods**
NAME                         READY     STATUS    RESTARTS   AGE
webserver-4288715076-dk9c7   1/1       Running   0          2m

$ **kubectl describe pod/webserver-4288715076-dk9c7**
Name:           webserver-4288715076-dk9c7
Namespace:      default
Node:           node/172.17.0.128
...
Status:         Running
IP:             10.32.0.2
Controllers:    ReplicaSet/webserver-4288715076
Containers:
  nginx:
    ...
    Ready:           True
    Restart Count:   0
    Liveness:        http-get http://:80/ delay=2s timeout=1s period=10s #...
...

Note that the output of the `kubectl describe` command has been edited down to the important bits; there’s much more information available but it’s not pertinent to our problem here.

## Discussion

In order to verify if a container in a pod is healthy and ready to serve traffic, Kubernetes provides a range of health-checking mechanisms. Health checks, or _probes_ as they are called in Kubernetes, are defined on the container level, not on the pod level, and are carried out by two different components:

*   The `kubelet` on each worker node uses the `livenessProbe` directive in the spec to determine when to restart a container. These liveness probes can help overcome ramp-up issues or deadlocks.
    
*   A service load balancing a set of pods uses the `readinessProbe` directive to determine if a pod is ready and hence should receive traffic. If this is not the case, it is excluded from the service’s pool of endpoints. Note that a pod is considered ready when all of its containers are ready.
    

When should you use which probe? That depends on the behavior of the container, really. Use a liveness probe and a `restartPolicy` of either `Always` or `OnFailure` if your container can and should be killed and restarted if the probe fails. If you want to send traffic to a pod only when it’s ready, use a readiness probe. Note that in this latter case, the readiness probe can be the same as the liveness probe.

## See Also

*   [Configure Liveness and Readiness Probes](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
    
*   [Pod Lifecycle documentation](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/)
    
*   [Init Containers documentation](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) (stable in v1.6 and above)